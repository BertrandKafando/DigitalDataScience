{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e25afc",
   "metadata": {},
   "source": [
    "L’objectif de ce TP est de programmer et de tester deux algorithmes de classification, très simples mais très efficaces : l’algorithme du Plus Proche Voisin (PPV) et le Classifieur Bayesien Naïf (CBN). Nous n’étudions ici que les versions les plus simple de ces algorithmes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f64ea6c",
   "metadata": {},
   "source": [
    "# Plus Proche Voisin Nearest Neighbor Classifier NNC \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94766b",
   "metadata": {},
   "source": [
    "\n",
    "1. Créez une fonction PPV(X,Y) qui prend en entrée des données X et des étiquettes Y et qui renvoie une étiquette, pour chaque donnée, prédite à partir du plus proche voisin de cette donnée. Ici on prend chaque donnée, une par une, comme donnée de test et on considère toutes les autres comme données d’apprentissage. Cela nous permet de tester la puissance de notre algorithme selon une méthode de validation par validation croisée (cross validation) de type “leave one out”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4a3fe938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3bfbf26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PPV(X, Y):\n",
    "    predicted_labels = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        # Utilisez toutes les données sauf la i-ème comme données d'apprentissage\n",
    "        X_train = np.delete(X, i, axis=0)\n",
    "        Y_train = np.delete(Y, i, axis=0)\n",
    "\n",
    "        # Calculez les distances euclidiennes entre la donnée de test et les données d'apprentissage\n",
    "        distances = euclidean_distances(X[i].reshape(1, -1), X_train)\n",
    "\n",
    "        # Trouvez l'indice de la donnée d'apprentissage la plus proche\n",
    "        closest_id = np.argmin(distances)\n",
    "\n",
    "        # Obtenez l'étiquette de la donnée la plus proche\n",
    "        predicted_label = Y_train[closest_id]\n",
    "\n",
    "        # Ajoutez l'étiquette prédite à la liste des étiquettes prédites\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    return np.array(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c02ee",
   "metadata": {},
   "source": [
    "\n",
    "2. La fonction PPV calcule une étiquette prédite pour chaque donnée. Modifiez la fonction pour calculer et renvoyer l’erreur de prédiction : c’est à dire le pourcentage d’étiquettes mal prédites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a14e2157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPV(X, Y):\n",
    "    predicted_labels = []\n",
    "    errors = 0\n",
    "    for i in range(len(X)):\n",
    "        # Utilisez toutes les données sauf la i-ème comme données d'apprentissage\n",
    "        X_train = np.delete(X, i, axis=0)\n",
    "        Y_train = np.delete(Y, i, axis=0)\n",
    "\n",
    "        # Calculez les distances euclidiennes entre la donnée de test et les données d'apprentissage\n",
    "        distances = euclidean_distances(X[i].reshape(1, -1), X_train)\n",
    "\n",
    "        # Trouvez l'indice de la donnée d'apprentissage la plus proche\n",
    "        closest_id = np.argmin(distances)\n",
    "\n",
    "        # Obtenez l'étiquette de la donnée la plus proche\n",
    "        predicted_label = Y_train[closest_id]\n",
    "\n",
    "        # Ajoutez l'étiquette prédite à la liste des étiquettes prédites\n",
    "        predicted_labels.append(predicted_label)\n",
    "        if (predicted_label !=Y[i]):\n",
    "            errors = errors + 1\n",
    "        \n",
    "    err = errors / len(Y)*100\n",
    "\n",
    "    return np.array(predicted_labels),err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62bbbb2",
   "metadata": {},
   "source": [
    "3. Testez sur les données Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9e5c36a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur en %: 4.0\n",
      "Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "ppv_resultat,error = PPV(X, Y) \n",
    "\n",
    "print(\"Erreur en %:\",error)\n",
    "print(\"Predicted labels: \",ppv_resultat)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b28ee4f",
   "metadata": {},
   "source": [
    "4. Testez la fonction des K Plus Proches Voisins de sklearn (avec ici K = 1). Les résultats sont-ils différents ? Testez avec d’autres valeurs de K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d2e44e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(X,Y)\n",
    "sk_resultat = neigh.predict(X)\n",
    "sk_resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a3059820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Is equal ? False\n"
     ]
    }
   ],
   "source": [
    "# Les résultats sont-ils différents \n",
    "print(\"\\n Is equal ? \"+str(np.array_equal(sk_resultat, ppv_resultat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04de19e6",
   "metadata": {},
   "source": [
    " Testez avec d’autres valeurs de K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2cd6eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      " Is equal ? False\n"
     ]
    }
   ],
   "source": [
    "#K=2\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(X,Y)\n",
    "sk_resultat = neigh.predict(X)\n",
    "print(sk_resultat)\n",
    "print(\"\\n Is equal ? \"+str(np.array_equal(sk_resultat, ppv_resultat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "be3e2618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      " Is equal ? True\n"
     ]
    }
   ],
   "source": [
    "#K=3\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X,Y)\n",
    "sk_resultat = neigh.predict(X)\n",
    "print(sk_resultat)\n",
    "print(\"\\n Is equal ? \"+str(np.array_equal(sk_resultat, ppv_resultat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9462cdf",
   "metadata": {},
   "source": [
    "Les résultats sont égales pour K=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32546d77",
   "metadata": {},
   "source": [
    "\n",
    "5. BONUS : Modifiez la fonction PPV pour qu’elle prenne en entrée un nombre K de voisins (au lieu de 1). La classe prédite sera alors la classe majoritaire parmi les Kvoisins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a7fa1d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage d'erreurs de prédiction : 5.333333333333334\n",
      "predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "def PPVMK(X, Y, K=1):\n",
    "    predicted_labels = []\n",
    "    errors = 0\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        # Utilize all data except the i-th as training data\n",
    "        X_train = np.delete(X, i, axis=0)\n",
    "        Y_train = np.delete(Y, i, axis=0)\n",
    "\n",
    "        # Calculate Euclidean distances between the test data and training data\n",
    "        distances = euclidean_distances(X[i].reshape(1, -1), X_train)\n",
    "\n",
    "        # Find the indices of the K nearest neighbors\n",
    "        closest_ids = np.argpartition(distances, K)[:,:K]\n",
    "\n",
    "        # Get the labels of the K nearest neighbors\n",
    "        k_nearest_labels = Y_train[closest_ids]\n",
    "\n",
    "        # Find the majority class among the K nearest neighbors\n",
    "        predicted_label, _ = mode(k_nearest_labels, axis=1, keepdims=True)\n",
    "\n",
    "        # Add the predicted label to the list of predicted labels\n",
    "        predicted_labels.append(predicted_label[0][0])  # Access the actual predicted label\n",
    "\n",
    "        # Check for errors\n",
    "        if predicted_label != Y[i]:\n",
    "            errors += 1\n",
    "\n",
    "    error_rate = errors / len(Y) * 100\n",
    "\n",
    "    return list(predicted_labels), error_rate\n",
    "\n",
    "predicted_labels,error = PPVMK(X, Y, 2)\n",
    "print(\"Pourcentage d'erreurs de prédiction :\", error)\n",
    "print(\"predictions:\",predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8db65",
   "metadata": {},
   "source": [
    "# Classifieur Bayesien Naïf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c9f1d",
   "metadata": {},
   "source": [
    "1. Créez une fonction CBN(X,Y) qui prend en entrée des données X et des étiquettes Y et qui renvoie une étiquette, pour chaque donnée, prédite à partir de la classe la plus probable selon l’équation (1). Ici encore, on prend chaque donnée, une par une, comme donnée de test et on considère toutes les données comme données d’apprentissage. Il est conseillé de calculer d’abord les barycentres et les probabilités à priori P (ωk) pour chaque classe, puis de calculer les probabilités conditionnelles P (xi/ωk) pour chaque classe et chaque variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "85f25232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "def calculate_centroids_dict(X, Y):\n",
    "    classes = np.unique(Y)\n",
    "    centroids_dict = {}\n",
    "\n",
    "    for c in classes:\n",
    "        X_c = X[Y == c]\n",
    "        centroid = np.mean(X_c, axis=0)\n",
    "        centroids_dict[c] = centroid\n",
    "\n",
    "    return centroids_dict\n",
    "\n",
    "def calculate_prior_probs_dict(Y):\n",
    "    classes = np.unique(Y)\n",
    "    prior_probs_dict = {}\n",
    "\n",
    "    for c in classes:\n",
    "        prior_probs_dict[c] = np.sum(Y == c) / len(Y)\n",
    "\n",
    "    return prior_probs_dict\n",
    "\n",
    "def calculate_conditional_probs_euclidean(x_test, centroids):\n",
    "    # Initialize an empty dictionary to store conditional probabilities\n",
    "    conditional_probs_dict = {}\n",
    "\n",
    "    for key, centroid in centroids.items():\n",
    "        # Calculate Euclidean distance between x_test and current centroid\n",
    "        distance = euclidean_distances(x_test.reshape(1, -1), np.array(centroid).reshape(1, -1))[0][0]\n",
    "        conditional_probs_dict[key] = distance\n",
    "\n",
    "    # Normalize the distances to get conditional probabilities\n",
    "    total_distance = sum(conditional_probs_dict.values())\n",
    "    conditional_probs_dict = {key: distance / total_distance for key, distance in conditional_probs_dict.items()}\n",
    "\n",
    "    return conditional_probs_dict\n",
    "\n",
    "def CBN(X, Y):\n",
    "    predictions = []\n",
    "\n",
    "    # Pour chaque donnée de test\n",
    "    for i in range(len(X)):\n",
    "        X_train = np.delete(X, i, axis=0)  # Utiliser toutes les données sauf la i-ème pour l'apprentissage\n",
    "        Y_train = np.delete(Y, i)\n",
    "        x_test = X[i]\n",
    "\n",
    "        centroids = calculate_centroids_dict(X_train, Y_train)\n",
    "        prior_probs = calculate_prior_probs_dict(Y_train)\n",
    "        conditional_probs = calculate_conditional_probs_euclidean(x_test, centroids)\n",
    "\n",
    "        # Multiply the conditional probabilities with prior probabilities\n",
    "        prediction = {key: prior_probs[key] * conditional_probs[key] for key in prior_probs}\n",
    "\n",
    "        # Get the class with the maximum value as the predicted class\n",
    "        predicted_class = max(prediction, key=prediction.get)\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b11580",
   "metadata": {},
   "source": [
    "2.La fonction CBN calcule une étiquette prédite pour chaque donnée. Modifiez la fonction pour calculer et renvoyer l’erreur de prédiction : c’est à dire le pourcentage d’étiquettes mal prédites. Testez sur les données Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b4f4fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CBN(X, Y):\n",
    "    errors = 0\n",
    "    predictions = []\n",
    "    # Pour chaque donnée de test\n",
    "    for i in range(len(X)):\n",
    "        X_train = np.delete(X, i, axis=0)  # Utiliser toutes les données sauf la i-ème pour l'apprentissage\n",
    "        Y_train = np.delete(Y, i)\n",
    "        x_test = X[i]\n",
    "\n",
    "        centroids = calculate_centroids_dict(X_train, Y_train)\n",
    "        prior_probs = calculate_prior_probs_dict(Y_train)\n",
    "        conditional_probs = calculate_conditional_probs_euclidean(x_test, centroids)\n",
    "\n",
    "        # Multiply the conditional probabilities with prior probabilities\n",
    "        prediction = {key: prior_probs[key] * conditional_probs[key] for key in prior_probs}\n",
    "\n",
    "        # Get the class with the maximum value as the predicted class\n",
    "        predicted_class = max(prediction, key=prediction.get)\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "        # Check if the predicted class is different from the actual class\n",
    "        if predicted_class != Y[i]:\n",
    "            errors += 1\n",
    "\n",
    "    # Calculate the percentage of errors\n",
    "    error_percentage = (errors / len(X)) * 100\n",
    "\n",
    "    return predictions,error_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3ac6f817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage d'erreurs de prédiction : 100.0\n",
      "Predictions: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "predicted_labels,error_percentage = CBN(X, Y)\n",
    "print(\"Pourcentage d'erreurs de prédiction :\", error_percentage)\n",
    "print(\"Predictions:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039bd17d",
   "metadata": {},
   "source": [
    "3. Testez la fonction du Classifieur Bayesien Naïf inclut dans sklearn. Cette fonction utilise une distribution Gaussienne au lieu des distances aux barycentres. Les résultats sont-ils différents ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6116e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "def CBNSKlearn(X, Y):\n",
    "    predictions_gaussian = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        X_train = np.delete(X, i, axis=0)\n",
    "        Y_train = np.delete(Y, i)\n",
    "        X_test = X[i]\n",
    "        true_label = Y[i]\n",
    "\n",
    "        # Utiliser GaussianNB\n",
    "        clf = GaussianNB()\n",
    "        clf.fit(X_train, Y_train)\n",
    "        pred_gaussian = clf.predict([X_test])[0]\n",
    "        predictions_gaussian.append(pred_gaussian)\n",
    "\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "    return predictions_gaussian, true_labels\n",
    "\n",
    "# Validation croisée avec GaussianNB et le classifieur personnalisé\n",
    "pred_gaussian, true_labels = CBNSKlearn(X, Y)\n",
    "\n",
    "print(\"Predicted Labels:\", pred_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2f0b7eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Is equal ? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n Is equal ? \"+str(np.array_equal(pred_gaussian, predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56581a8e",
   "metadata": {},
   "source": [
    "Les résultats sont différents, la fonction avec une distribution gaussienne  est nettement meilleure que le premier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6626aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def calculate_conditional_probs_gaussian(x_test, centroids,x_train):\n",
    "    # Initialize an empty dictionary to store conditional probabilities\n",
    "    conditional_probs_dict = {}\n",
    "\n",
    "    for key, centroid in centroids.items():\n",
    "        # Calculate the mean and covariance matrix for the Gaussian distribution\n",
    "        mean = centroid\n",
    "        cov_matrix = np.cov(np.transpose(x_train))  # Assuming x_train is available\n",
    "\n",
    "        # Create a multivariate normal distribution\n",
    "        mvn = multivariate_normal(mean=mean, cov=cov_matrix)\n",
    "\n",
    "        # Calculate the probability density for x_test\n",
    "        prob_density = mvn.pdf(x_test)\n",
    "\n",
    "        conditional_probs_dict[key] = prob_density\n",
    "\n",
    "    return conditional_probs_dict\n",
    "\n",
    "def CBN(X, Y):\n",
    "    predictions = []\n",
    "\n",
    "    # Pour chaque donnée de test\n",
    "    for i in range(len(X)):\n",
    "        X_train = np.delete(X, i, axis=0)  # Utiliser toutes les données sauf la i-ème pour l'apprentissage\n",
    "        Y_train = np.delete(Y, i)\n",
    "        x_test = X[i]\n",
    "\n",
    "        centroids = calculate_centroids_dict(X_train, Y_train)\n",
    "        prior_probs = calculate_prior_probs_dict(Y_train)\n",
    "        conditional_probs = calculate_conditional_probs_gaussian(x_test, centroids,X_train)\n",
    "\n",
    "        # Multiply the conditional probabilities with prior probabilities\n",
    "        prediction = {key: prior_probs[key] * conditional_probs[key] for key in prior_probs}\n",
    "\n",
    "        # Get the class with the maximum value as the predicted class\n",
    "        predicted_class = max(prediction, key=prediction.get)\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Utiliser les fonctions pour prédire les étiquettes\n",
    "predicted_labels = CBN(X, Y)\n",
    "print(\"Predicted Labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41223d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7898cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b6c62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b75eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
